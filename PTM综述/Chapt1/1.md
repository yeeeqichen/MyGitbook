# 相关资料列表



### 综述
- Don't Stop Pretraining: Adapt Language Models to Domains and Tasks
- Pre-Trained Models: Past, Present and Future

### 预训练模型资源

- Hugging Face

### 无监督任务

![](D:\gitbook\PTM综述\pictures\预训练任务目标.png)


### 预训练模型列表

- BERT及其变种
- GPT及其变种
- ERNIE及其变种
- 等等



### BERT瓶颈：

训练时间长

复杂度高，相对于序列长度二次，并且应用于生成模型时attention计算代价大大提高（不仅二次复杂度，而且无法并行）

- Finetuning Pretrained Transformers into RNNs  替换BERT中的attention相似度计算方法来加速，变成RNN-like，每次计算复用之前的状态值，在长序列状况下大大减少时空复杂度
- GOOGLE提出：LAMB优化器 + 超大batch 



### To Pretrain or Not to Pretrain: Examining the Beneﬁts of Pretraining on Resource Rich Tasks 

- 'Our ﬁndings indicate that MLM-based models might reach a diminishing return point as the supervised data size increases signiﬁcantly.'  当标注训练数据到达一定的量级后，预训练模型的优势在消失

 ### BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension

- ‘A key advantage of this setup is the noising ﬂexibility; arbitrary transformations can be applied to the original text, including changing its length.’ 使用seq-to-seq架构带来的好处
- randomly shuffling the order of theoriginal sentences
- arbitrary length spans of text (including zero length) are replaced with a single mask token


### SpanBERT: Improving Pre-training by Representing and Predicting Spans
- SBO：直接依赖两端的boundary信息预测整个Span，不借助于span内的信息
- 不再用Next Sentence Prediction
- L(xi) = LMLM(xi) + LSBO(xi) loss由两部分组成，传统MLM + SBO

### UNIMO: Towards Unified-Modal Understanding and Generation via Cross-Modal Contrastive Learning
- Single-modal && Multi-modal 三个预训练过程：Visual、Text、Visual + Text
- 利用大量的单模态信息（图片、文本）来辅助多模态训练，在多模态中对文本部分进行数据增强
- 文本预训练方面：‘All tokens in the selected spans are replaced with either a special [MASK] token, a random token or the original token’ 结合 span 、[mask]、替换



### XLNet: Generalized autoregressive pretraining for language understanding.
- AR(Auto Regressive)模型，指出BERT没有考虑[MASK]之间的联系
- Permutation，相较于前向+后向的改进
  - 思想：保留AR做法，适用于Generation-like任务，但是通过预训练的trick来将下文信息揉进模型当中
  - 一种看法：其实就是在Transformers里面作类似随机MASK，而BERT是在输入端显式地进行mask
- 双流注意力机制（具体做法没细看）

### GPT
- 训练目标：Task irrelevant，应用于zero-shot场景
- AR模型

### DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter

- 砍掉一半的BERT layers，去掉pooler层和position embedding层
- student-teacher learning，loss项加入与teacher的差异loss
- 提速60%，模型缩小40%，表现基本持平
- 使用原模型的参数进行初始化（“taking one layer out of two")


### T5， Exploring the Limits of Transfer Learning with a Uniﬁed Text-to-Text Transformer
- 将所有问题重新建模成text-to-text，问答、摘要、翻译等等，将任务目标转化为文字描述加入到模型输入当中
- 分析了预训练方法、模型结构、微调方法、计算代价等各种因素，进行了综合探究
- 船新超大数据集：C4
- 大力出奇迹还是提升模型表现的好办法
- 结合当前研究与应用情况，提出了几个展望和总结

Electra



### RoBERTa: A Robustly Optimized BERT Pretraining Approach
- 原有的BERT："Significanly undertrained"
- 大力出奇迹：更多数据、更大batch、更长训练时间
- 去掉Next-Sentence-Prediction 任务（后续的很多模型都这么做了)，怀疑是原BERT只删除了loss，保留拼接不同文章sentence的输入，导致模型表现变差
- 使用更长的序列进行训练
- Dynamically Masking Strategy




### 预训练模型优缺点总结



### 预训练模型瘦身
- Distillation
- Quantization
- Pruning

### 预训练模型应用
- BERT 掩码填词
- Electra 作命名实体识别 
- GPT-2 作文本生成 （完成）
- RoBERTa 作自然语言推理
- BART 作文本摘要
- DistilBERT作问答
- 用 T5做翻译

### 预训练模型微调方法
- 相关论文：
  - The Power of Scale for Parameter-Efficient Prompt Tuning
  - PPT: Pre-trained Prompt Tuning for Few-shot Learning

- 方法一：在模型上加一个task-specific head，对整体/head only 进行微调
- 方法二：使用prompt，将问题重新建模为text-to-text，对整体/prompt微调，对于超大型模型，仅微调prompt可以大大提高使用效率，同时为了适应few-shot使用场景，可以设计预训练任务来获得预训练的prompt（套娃？）



### Fine-tune 细节
- batchsize
- LR 策略
- 等等
