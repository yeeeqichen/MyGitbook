# 相关资料列表



### 综述
- Don't Stop Pretraining: Adapt Language Models to Domains and Tasks
- Pre-Trained Models: Past, Present and Future



### 预训练模型列表

- BERT及其变种
- GPT及其变种
- ERNIE及其变种
- 等等



### BERT瓶颈：

训练时间长

复杂度高，相对于序列长度二次，并且应用于生成模型时attention计算代价大大提高（不仅二次复杂度，而且无法并行）

- Finetuning Pretrained Transformers into RNNs  替换BERT中的attention相似度计算方法来加速，变成RNN-like，每次计算复用之前的状态值，在长序列状况下大大减少时空复杂度
- GOOGLE提出：LAMB优化器 + 超大batch 



### To Pretrain or Not to Pretrain: Examining the Beneﬁts of Pretraining on Resource Rich Tasks 

- 'Our ﬁndings indicate that MLM-based models might reach a diminishing return point as the supervised data size increases signiﬁcantly.'  当标注训练数据到达一定的量级后，预训练模型的优势在消失

 ### BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension

- ‘A key advantage of this setup is the noising ﬂexibility; arbitrary transformations can be applied to the original text, including changing its length.’ 使用seq-to-seq架构带来的好处
- randomly shuffling the order of theoriginal sentences
- arbitrary length spans of text (including zero length) are replaced with a single mask token


### SpanBERT: Improving Pre-training by Representing and Predicting Spans
- SBO：直接依赖两端的boundary信息预测整个Span，不借助于span内的信息
- 不再用Next Sentence Prediction
- L(xi) = LMLM(xi) + LSBO(xi) loss由两部分组成，传统MLM + SBO

### UNIMO: Towards Unified-Modal Understanding and Generation via Cross-Modal Contrastive Learning
- Single-modal && Multi-modal 三个预训练过程：Visual、Text、Visual + Text
- 利用大量的单模态信息（图片、文本）来辅助多模态训练，在多模态中对文本部分进行数据增强
- 文本预训练方面：‘All tokens in the selected spans are replaced with either a special [MASK] token, a random token or the original token’ 结合 span 、[mask]、替换



### XLNet: Generalized autoregressive pretraining for language understanding.
- AR(Auto Regressive)模型，指出BERT没有考虑[MASK]之间的联系
- Permutation，相较于前向+后向的改进
  - 思想：保留AR做法，适用于Generation-like任务，但是通过预训练的trick来将下文信息揉进模型当中
  - 一种看法：其实就是在Transformers里面作类似随机MASK，而BERT是在输入端显式地进行mask
- 双流注意力机制（具体做法没细看）

### GPT

​	看GPTpapaer + code



